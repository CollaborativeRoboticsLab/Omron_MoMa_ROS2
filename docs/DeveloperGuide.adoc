// To add: what packages are needed


= MoMa ROS 2 - Developer Guide
:site-section: DeveloperGuide
:toc:
:toclevels: 3
:toc-title: Table of Contents
:toc-placement: preamble
:icons: font
:sectnums:
:imagesDir: images
:librariesDir:
:stylesDir: stylesheets
:xrefstyle: full
:experimental:
:linkattrs:
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:warning-caption: :warning:
endif::[]

:url-repo: https://github.com/guanyewtan/Omron_TM_ROS2
:url-ug: https://github.com/guanyewtan/Omron_TM_ROS2

Last updated: `2 July 2021` 

Authors: `Tan Guan Yew`(link:https://github.com/guanyewtan[guanyewtan]) 


== Getting Started
[[prerequisites]]
=== Prerequisites


. **Submodules**
+
This package contains 2 submodules (see link:https://git-scm.com/book/en/v2/Git-Tools-Submodules[git submodules]) and the `omron_moma` package.
+
The 2 submodules are: 
+
.. link:https://github.com/guanyewtan/Omron_TM_ROS2[Omron_TM_ROS2]
+ 
This submodule manages the TM robot, and contains packages to control and visualise the robot model in RViz.
+
.. link:https://github.com/guanyewtan/Omron_AMR_ROS2.git[Omron_AMR_ROS2]
+
This submodule manages the Autonomous Mobile Robot (AMR), and contains packages to control and visualise the robot model in RViz.

These packages can be used individually to control the TM machine and Autonomous Mobile Robot respectively, and the `omron_moma` package utilises both of them to control the Mobile Manipulator.

In order to run the `omron_moma` package, *ensure that the requirements and setup for running Omron_TM_ROS2 (link:https://github.com/guanyewtan/Omron_TM_ROS2/blob/master/docs/DeveloperGuide.adoc[Developer Guide]) and Omron_AMR_ROS2 (link:https://github.com/guanyewtan/Omron_AMR_ROS2/blob/master/docs/DeveloperGuide.adoc[Developer Guide]) have been met.*


=== Hardware Connection
The diagram below shows the hardware connection of the MoMa. 

.Hardware Connection
image::hardware_connection.png[]

The Intel NUC, AMR and TM robot are all connected to the ethernet switching hub via ethernet cable. *Ensure that the 3 machines are able to communicate with each other on the same subnet, e.g. 192.168.1.x with subnet mask 255.255.255.0.*

=== ROS2 Across Multiple Machines
The `omron_moma` package has been designed to run across 2 machine; the host machine, which runs the nodes for communication with the TM robot and the AMR, as well as the remote machine, which runs the high level scripts/clients to execute jobs on the MoMa. 

To set up ROS2 across the host and remote machine, ensure that both the remote machine and host machine are on the same subnet and are discoverable to each other.

*Also ensure that the ROS_DOMAIN_ID of both devices are the same.*


=== Set Up Host Machine
The host machine is used to run the nodes which enable communication with the TM robot via tcp and modbus communication.
It is also used to communicate with the AMR via the ARCL interface in order to retrieve vital information about the AMR that is used for this package to work.


==== Host Machine ROS 2 Package Set Up
Once you have your network set up correctly, you need to set up our ROS 2 package to work correctly in your host machine.

First, make sure you have installed ROS 2 as described in <<prerequisites>>.

. Clone this repository to a directory of your choice with: 
+
....
cd <directory>
git clone --recurse-submodules https://github.com/zach-goh/Omron_MoMa_ROS2.git
....
+
[NOTE]
If you pass --recurse-submodules to the git clone command, it will automatically initialize and update each submodule in the repository.
. Enter the folder with:
+
....
cd Omron_MoMa_ROS2
....
. Build all package and source the setup.bash file:
+
....
colcon build --symlink-install
source install/setup.bash
....
+
[NOTE]
Depending on your machine, this can take a while to build.
If you receive a warning saying "no such command", follow the intructions link:https://docs.ros.org/en/foxy/Tutorials/Colcon-Tutorial.html#install-colcon[here].
You might need to install some missing packages if you didn't already have them. They can be installed with `sudo apt install ros-foxy-control-msgs`.
+
. Run the nodes for the host machine:
+
....
ros2 launch omron_moma server.launch.py robot_ip:=<ip address of TM machine>
....
+
This will launch the nodes for communication with the TM and the AMR.

// ==== TM ROS 2 Driver Setup/Usage
// 
// . Enter your ROS 2 workspace and source the ROS2 environment:
// +
// ```
// source /opt/ros/foxy/setup.bash
// cd <workspace>
// source ./install/setup.bash
// ```

// . Ensure that TM Robot's operating software (__TMflow__) system/network settings have been set and the __Listen node__ is running (run the project above)

// . Run the driver to maintain the connection with TM Robot:
// +
// ```
// ros2 run tm_driver tm_driver <robot_ip_address>
// ```
// +
// Example: `ros2 run tm_driver tm_driver 192.168.2.10`, if the <robot_ip_address> is 192.168.2.10
// +
// Now, the user can use a __new terminal__ to run each ROS node or command, but don't forget to source the correct setup shell files afteras starting a new terminal!
// +
// The TM driver is required to be running so long as a connection to the listen node of the TMflow program is required.
// +
// For more information on the TM Drivers, click link:https://github.com/TechmanRobotInc/tmr_ros2/blob/master/README.md[here].



// == Software Design
// [[architecture]]
// === Architecture
// An overview of this package architecture is summarised in the diagram below:

// .Overview of package
// image::SoftwareOverview.png[]

// === Socket TCPlistener
// Users can establish a socket TCPlistener in the listen node to connect to external device and communicate based on the packet format.

// All features available in TM ROBOT Function can be operated in the listen node. For more information on the listen node, please refer to page 181 of the __TM Expression Editor and Listen Node Reference Guide__.

// The TM Driver utilises TMSCT and TMSTA communication packages to send external scripts and obtain status or properties of the TM respectively. Below is an example of how the TM Driver uses these 2 communication packages to communicate through the Socket TCPlistener:

// . TM Driver sends a PTP (point-to-point) movement command via a ROS2 service client using TMSCT packages to the TM Robot. When the command has been successfully sent, an acknowledgement is sent back to the host machine.

// . A queue tag is sent via TMSCT communication packages and its status monitored using TMSTA, to check if a motion command has been completed.

// === Modbus
// Users can use Modbus Client to read or write the parameters and save them in the robot register, such as position, posture and IO status. Users can program with the obtained parameters or monitor the status of robot. TM Robot provides two protocol versions of Modbus: Modbus TCP and Modbus RTU for users to get data from the external Modbus device or robot register, but TCP is being used for these packages.

// .Modbus Protocol
// image::ModbusProtocol.png[]

// There are limitations to the capabilities of the TM Drivers, which is why Modbus is being used to send and receive information unobtainable by the drivers, such as getting the coordinates of the current base in the project flow or starting the project from outside the listen node.

// The pymodbus libraries are used to communicate with the modbus servers.


// === RViz Visualisation
// The `RViz` package allows a 3D model TM robot to be displayed in a separate window for real time visualisation.

// This package uses the joint states generated from the TM driver as well as the robot description publisher to generate the model and display it in its current position.

// To understand how `RViz Visualisation` is structured with the entire ROS package and communicates with LD, see <<architecture>>.

// `RViz Visualisation` has three nodes, they are summarised as below:

// [cols="1,1a", options="header"]
// .LD Visualisation nodes
// |===
// |**Node name**
// |**Description**

// |tm_driver
// |
// This node is responsible for publishing the joint states that the RViz program subscribes to.

// Using this information, it updates the position of the TM shown on RVIZ.

// |robot_description
// |
// This node is responsible for pubishing the .urdf information that the RViz program uses to display the model of the robot, as well as know the transform of each component of the robot relative the another point.

// |static_transform_publisher
// |
// This node is responsible for publishing a transform which sets the base of the 3D model to the zero coordinate.

// |pp_marker
// |
// This node is responsible for publishing a transform and marker which represents an object picked up by the grippers.

// |===


== Implementation
=== Demo Program
The omron_moma package allows the user to create a load and unload program, requiring only a one time setup. The demo program will then run a vision guided load and unload operation at 2 different goals.

*To run the demo program, ensure that:*

. The requirements in <<prerequisites>> have been met.

. `server.launch.py` has been launched on the host machine.

. There are 2 goals set for the AMR, one called 'Goal1' and the other called 'Goal2'.

. The load and unload motion has been taught. This can be done by running
+
....
ros2 run omron_moma teach_setup <ip address of TM>
....
+
[NOTE]
This teach_setup.py file *is different from the one in the pickplace package*. This one requires the user to enter the name of the goal that the pick and place motion will be carried out at. For example, if the pick and place motion is done at Goal1, the user will be prompted to enter 'Goal1' when running the setup. Everything else works the same.


==== Running the demo program

. Enter the folder with:
+
....
cd Omron_MoMa_ROS2
....
. Build all package and source the setup.bash file:
+
....
colcon build --symlink-install
source install/setup.bash
....
+
. *Make sure the TM program is running, either in auto or manual mode*
+
. Run the demo script:
+
....
ros2 run omron_moma demo <ip address of TM>
....

The MoMa should move to Goal2, execute a pick and place motion, then move to Goal1 and execute a pick and place motion.

// [NOTE]
// This implementation was designed using a TM Landmark attached to the object to be picked, and another TM landmark to find the place location. However, it is possible for the TM vision to locate other visual features on objects.

// These 2 landmarks act as the vision base for the pick and place, and the vision job takes into account the tilt and rotation of the TM landmark (so long as it can be seen clearly by the camera)

// There are 2 stages to the program: a setup phase, where the user sets the location of TM to view the pick and place landmarks as well as the pick and place locations, and an execution phase, where the pick and place operation will run based on the coordinates set in the setup phase.

// ==== Setup
// The teach_setup.py script runs through a sequence of instructions for the user to record the pick and place positions of an object, as well as the positions to view their respective landmarks. The following diagram shows the flow of the setup program:

// .Pickplace setup flow
// image::teachsetup2.png[]

// . Run the program to teach the setup, replacing `robot_ip_address` with the ip address of the TM robot.
// +
// ....
// ros2 run pickplace teach_setup <robot_ip_address>
// ....
// +
// . Move the robot to an initial position to start with on program launch, then press kbd:[ENTER]
// +
// image::prompt6.png[]
// +
// . Enter the name of the vision base that will be created in the TMflow program by the vison job.
// +
// image::prompt7.png[]
// +
// image::visionjobname.png[]

// [NOTE]
// The vision base name is the name of the vision job with `vision_` appended to the front. For example, if the vision job name is `myvisionjob`, the vision base name will be `vision_myvisionjob`.

// . Move the TM arm to the landmark viewing position for picking the object.
// +
// image::viewpick.gif[]
// +
// . Press the play button on the robot stick. You should hear the robot beep 3 times to signal the start of the TMflow project.
// +
// image::robotstick_play.jpg[]
// +
// . Hit kbd:[ENTER] to start the vision job. 
// +
// image::prompt2.png[]
// +
// . Wait for the vision job to complete, then press the stop button on the robot stick. The robot should beep once.
// [NOTE]
// It is advised to open up the vision job to check if the landmark can be detected, or this might cause problems with detection during the execution process.
// +
// image::robotstick_stop.jpg[]
// +
// . Move the TM arm to the pick position.
// +
// image::pick.gif[]
// +
// [WARNING]
// Ensure that the object remains in the SAME POSITION as it was during the vision job.
// +
// . Hit kbd:[ENTER] to close the grippers and record the position, then move the TM arm to the landmark viewing position for placing the object.
// +
// image::prompt3.png[]
// +
// image::viewplace.gif[]
// +
// . Press the play button on the robot stick. The robot should beep 3 times.
// . Hit kbd:[ENTER] to start the vision job.
// [NOTE]
// It is advised to open up the vision job to check if the landmark can be detected, or this might cause problems with detection during the execution process.
// . When the vision job is done, press the stop button on the robot stick. The robot should beep once.
// +
// image::prompt4.png[]
// +
// . Move the TM arm to the place position and hit kbd:[ENTER], which releases the grippers and records the position.
// +
// image::prompt5.png[]
// +
// image::place.gif[]
// +
// . A JSON file will be generated containing the coordinates of the landmark viewing positions, the pick and lace positions as well as the name of the vision job, to be used in the execution stage.

// ==== Execution

// The pickplace_program.py script takes the coordinates obtained from the setup stage and implements a continuous pick and place program, with an RViz terminal showing the real time pose of a 3D model of the TM robot. The flow of the program is shown below:

// .Pickplace execution flow
// image::pickplaceflow.png[]

// . . Run the launch file to execute the program, replacing `robot_ip_address` with the ip address of the TM robot.
// +
// ....
// ros2 launch pickplace pickplace.launch.py robot_ip:=<robot_ip_address>
// ....
// +
// This will run the pick & place program as well as the RViz window for the 3D model visualisation.
// +
// image::pickplace.gif[]
// image::rviz3.png[]
// +
// If you want a marker to show up when the robot picks up an item, add a topic to RViz called "marker"

// === Class Diagram

// .Class Relations diagram
// image::classrelation.png[]

// [CAUTION]
// The `Script` class is *not* the pickplace program's python script, it is a class that handles the scripts to be sent to the TCP socket listener in the TM machine via the TM driver.

// ==== Move Class

// .Explanation of Move Class
// image::moveclass.png[]

// The movement of the TM arm is controlled by the Move class. The `set_position` method takes in x, y, z, roll, pitch and yaw, and sends it to the listen node in the TMflow program via a service client from the TM Driver. 

// [NOTE]
// Queue tags and status queries were also used to ensure that no other command executes simultaneously until the motion is complete. For more information, please refer to section 8.1 of the __TM Expression Editor and Listen Node Reference Guide__ under Robot Motion Functions.

// ==== Transform Class

// .Explanation of Transform Class
// image::transformclass1.png[]
// .More explanations of Transform Class
// image::transformclass2.png[]

// This class handles all the transforms between different frames (e.g. coordinates of the pick position relative to the robot base or vision base). The TF tree is shown below:

// .TF Tree of pickplace program
// image::tftree2.png[]

// [NOTE]
// safe_pick and safe_place transforms are the coordinates of the pick and place point with a -0.1m offset in the z-axis

// ==== Script Class

// .Explanation of Script Class
// image::scriptclass.png[]

// The Script class uses the external scripts function of the TM Driver to run external commands.

// [NOTE]
// The script class is capable of sending instructions to run TM movement commands, however the TM SetPosition service client is preferred as the format of the parameters are more intuitive than a script implementation.

// ==== Modbus Class

// .Explanation of Modbus
// image::modbusclass.png[]

// The modbus class handles modbus communication between the client (TM machine) and server (user's computer). It allows for asynchronous control of many features of the TM robot.

// [NOTE]
// The TM Driver provides services to open and close the IO gripper. Howeveer, modbus control of the IO is preferred as it does not require the TM Driver to be running concurrently with the program, as running the TM Driver to control the IO would require the TMflow program to be in the listen node, which prevents the TM robot from being able to be manually moved to the setup locations during the setup stage.

